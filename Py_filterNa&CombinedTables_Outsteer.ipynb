{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oDgp_n_ZuAIWLfZtFOpqJ0jVioDu5blp",
      "authorship_tag": "ABX9TyPXFKXEXb7deA6FhgOdAa4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Austinxdsouza/2024-Python_SQL_projets/blob/main/Python%26PySparkApp_Outsteer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDZY-C_9wtID",
        "outputId": "eb928a29-6cd1-4de8-909a-f58f7efa6c29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=a2028a8ba4c84d136f14c7faccda3783c009b3060ea0435877ce0cda505cad85\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK ONE**\n",
        "\n",
        "Q1 (Python)\n",
        "\n",
        "Write a Python script to read a large text file (containing millions of rows) and filter out all the\n",
        "lines that contain the value 'N/A'.\n",
        "Then, write the filtered data to a new file.\n",
        "Make sure your script can\n",
        "\n",
        "- handle the large size of the input file without running out of memory.\n",
        "\n",
        "My Solution =\n",
        "1. Made use of Spark instead of Pandas so that the Workload is more distributed across its Nodes.\n",
        "2. Made use of show() instead of collect to reduce load on the Driver Node.\n",
        "3. Loaded the data into a seperate directory in Snappy-Parquet format which is an efficient format for storing Analytical/DW/Queriable Data.\n",
        "4. Also Stopped Spark engine in the end to save costs.\n",
        "\n",
        "- minimize i/o operations\n",
        "\n",
        "This will be like running grep input.txt |grep -v 'N/A' > output.txt\n",
        "\n",
        "My Solution=\n",
        "1. I have made use of Sparks Filter method which is distributed in nature while performing transformations\n",
        "\n",
        "filtered_na_df = df.filter(df[\"Name\"].isNull())\n"
      ],
      "metadata": {
        "id": "6uBIKqO2C2EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Rg6qITkuuffB",
        "outputId": "26ff1dce-083f-4486-bdff-035341e2aef9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/drive/MyDrive/Colab Notebooks/PythonApp_Outsteer/output/filtered_na_lines_df already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-99b3eee21da9>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Save the filtered data to a new file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfiltered_na_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#delta is slightly better than parquet in production environments because it has elements like Time Travel and a Transaction Layer Added for Transactional Efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/drive/MyDrive/Colab Notebooks/PythonApp_Outsteer/output/filtered_na_lines_df already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
        "\n",
        "input = \"/content/drive/MyDrive/Colab Notebooks/PythonApp_Outsteer/healthcare_dataset.csv\"\n",
        "output = \"/content/drive/MyDrive/Colab Notebooks/PythonApp_Outsteer/output/filtered_na_lines_df\"\n",
        "\n",
        "df = spark.read.option(\"header\",\"true\").csv(input)\n",
        "\n",
        "# Filter out lines that contain 'N/A'\n",
        "filtered_na_df = df.filter(df.Name.isNull() & df.Doctor.isNull()) # Just like this we can add all the relavant columns\n",
        "\n",
        "# Save the filtered data to a new file\n",
        "filtered_na_df.write.parquet(output)  #delta is slightly better than parquet in production environments because it has elements like Time Travel and a Transaction Layer Added for Transactional Efficiency\n",
        "\n",
        "\n",
        "# # Stop the Spark session\n",
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK TWO**\n",
        "Q2 (Pyspark)\n",
        "\n",
        "We have data stored in Azure Blob Storage partitioned by tenant name and short_date (YYYY-\n",
        "MM-DD format). There are two existing tables with this data:\n",
        "\n",
        "• **Events** – pulled daily by event_time (Columns: event_id, event_time, common_data_1, common_data_2)\n",
        "\n",
        "• **Updates** – pulled daily by last_modified (Columns: event_id, event_time, last_modified, common_data_1, specific_data_1)\n",
        "\n",
        "We want to create a new table called **updated_events** that combines the relevant information\n",
        "from both tables. This new table should include the following columns:\n",
        "\n",
        "1. event_id (key)\n",
        "2. event_time (key)\n",
        "3. last_modified\n",
        "4. common_data_1\n",
        "5. common_data_2\n",
        "6. specific_data_1\n",
        "\n",
        "Important keynotes:\n",
        "1. The updated_events table should reflect the **latest update** for that event_id and event_time.\n",
        "2. Pay attention to **performance** of the spark job.\n",
        "3. The spark job should **run daily** and fill updated_events with missing event/updates.\n",
        "\n",
        "Walk us through the process of creating the updated_events table using pyspark, considering the **data partitioning, update logic and backfill strategy.**\n"
      ],
      "metadata": {
        "id": "Xga7i7efwCLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.types\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import DataFrameWriter #for partitionBy()\n",
        "\n",
        "\n",
        "#Define Schema Manually\n",
        "eventsSchema = StructType([\n",
        "            StructField('event_id',IntegerType(),True),\n",
        "            StructField('event_time', IntegerType(),True),  #To speed up the process I used IntegerType but I understand logically it should be TimestampType().\n",
        "            StructField('common_data_1',StringType(),True),\n",
        "            StructField('common_data_2',StringType(),True)])\n",
        "\n",
        "updatesSchema = StructType([\n",
        "            StructField('event_id',IntegerType(),True),\n",
        "            StructField('event_time',IntegerType(),True),\n",
        "            StructField('last_modified',StringType(),True),\n",
        "            StructField('common_data_1',StringType(),True),\n",
        "            StructField('specific_data_1',StringType(),True)\n",
        "            ])\n",
        "\n",
        "# #Entering data manually\n",
        "\n",
        "EventsData = [Row(1000,1050, \"James\", \"abc\"),\n",
        "              Row(1001,1350, \"Jack\", \"abc\"),\n",
        "              Row(1002,1750, \"John\", \"anf\")]\n",
        "\n",
        "UpdatesData = [Row(1000,1050,2000, \"James1\", \"Mathew\"),\n",
        "              Row(1001,2300,2000, \"James1\", \"Mathew\")]\n",
        "\n",
        "\n",
        "EDf = spark.createDataFrame(EventsData,eventsSchema)  #Events_DataFrame\n",
        "EDf.createOrReplaceTempView(\"EventsDf\") #Optional\n",
        "UDf = spark.createDataFrame(UpdatesData,updatesSchema)  #Updates_DataFrame\n",
        "UDf.createOrReplaceTempView(\"UpdatesDf\")  #Optional\n",
        "\n",
        "  # USING PYSPARK\n",
        "\n",
        "Updated_EventsDF = EDf.join(UDf).where((EDf[\"event_id\"] == UDf[\"event_id\"]) &\n",
        "                  (EDf[\"event_time\"] == UDf[\"event_time\"]))\n",
        "# Join the two tables \"EventsData\" & \"Updates Data \"to form \"Updated_Events Data\"\n",
        "\n",
        "Updated_EventsDF.write.partitionBy(\"common_data_1\").delta(\"#Desired Deltalake Location\")\n",
        "#PartitionBy () using common_data_1 cause it seems most probably CATEGORICAL DATA and not RECURCIVE or CONTINUOUS\n",
        "\n",
        "\"\"\" WORKFLOW MANAGEMENT INFORMATION\n",
        "\n",
        "In order to run the notebook daily we have two options -\n",
        "\n",
        "1. Azure Data Factory as an Orchestration Tool:\n",
        "    Create an ETL Pipeline, Add the Notebook (as ADF Databricks Activity) that produces the Combined Results in a desired Fashion,\n",
        "    Set the Schedule Trigger frequency say Daily at 07:00.\n",
        "\n",
        "2. Databricks Notebook Workflows:\n",
        "\n",
        "  Databricks Jobs: Create New Job> ScheduleType Schedule>EveryDay at 07:00 Hrs\n",
        "      Select Task Type> Notebook, NotebookLocation>.., Cluster>SingleNodeLatestSparkVersion,\n",
        "      Set parameters(), Can set TimeOut/Retries/Alerts\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "8mGFUbbEwAdV",
        "outputId": "86757024-f504-4b18-d069-9c5a3672b3bc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-------------+-------------+--------+----------+-------------+-------------+---------------+\n",
            "|event_id|event_time|common_data_1|common_data_2|event_id|event_time|last_modified|common_data_1|specific_data_1|\n",
            "+--------+----------+-------------+-------------+--------+----------+-------------+-------------+---------------+\n",
            "|    1000|      1050|        James|          abc|    1000|      1050|         2000|       James1|         Mathew|\n",
            "+--------+----------+-------------+-------------+--------+----------+-------------+-------------+---------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' WORKFLOW MANAGEMENT \\n\\nIn order to run the notebook daily we have two options - \\n\\n1. Azure Data Factory as an Orchestration Tool:\\n    Create an ETL Pipeline, Add the Notebook (as ADF Databricks Activity) that produces the Combined Results in a desired Fashion, \\n    Set the Schedule Trigger frequency say Daily at 07:00.\\n\\n2. Databricks Notebook Workflows:\\n  \\n  Databricks Jobs: Create New Job> ScheduleType Schedule>EveryDay at 07:00 Hrs \\n      Select Task Type> Notebook, NotebookLocation>.., Cluster>SingleNodeLatestSparkVersion, \\n      Set parameters(), Can set TimeOut/Retries/Alerts\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** ROUGH NOTES**"
      ],
      "metadata": {
        "id": "SJ3J0jMqCxNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"CREATE TABLE Events (\n",
        "  event_id INT,\n",
        "  event_time TIME,\n",
        "  common_data_1 VARCHAR(255),\n",
        "  common_data_2 VARCHAR(255)\n",
        "  );\n",
        "\n",
        "CREATE TABLE Updates (\n",
        "  event_id INT,\n",
        "  event_time TIME,\n",
        "  last_modified VARCHAR(255),\n",
        "  common_data_1 VARCHAR(255),\n",
        "  specific_data_1 VARCHAR(255)\n",
        "  );\n",
        "\n",
        "--INSERT INTO Events(event_id, event_time,common_data_1,common_data_2) VALUES (10,CURRENT_TIMESTAMP,'ABC','EIF');\n",
        "--INSERT INTO Events(event_id, event_time,common_data_1,common_data_2) VALUES (11,CURRENT_TIMESTAMP,'ABE','EIK');\n",
        "\n",
        "INSERT INTO UPDATES(event_id, event_time,last_modified,common_data_1,specific_data_1) VALUES (11,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP-1,'ABC','XDX');\n",
        "\n",
        "SELECT * FROM UPDATES\n",
        "\n",
        "# MANUALLY DEFINING SCHEMA USING PYSPARK STRUCTTYPE\n",
        "\n",
        "import pyspark.sql.types\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "eventsSchema = StructType([\n",
        "            StructField('event_id',IntegerType(),True),\n",
        "            StructField('event_time', TimestampType(),True),\n",
        "            StructField('common_data_1',StringType(),True),\n",
        "            StructField('common_data_2',StringType(),True)])\n",
        "\n",
        "updatesSchema = StructType([\n",
        "            StructField('event_id',IntegerType(),True),\n",
        "            StructField('event_time',TimestampType(),True),\n",
        "            StructField('last_modified',StringType(),True),\n",
        "            StructField('common_data_1',StringType(),True),\n",
        "            StructField('specific_data_1',StringType(),True)\n",
        "            ])\n",
        "\n",
        "REF\n",
        "\n",
        "https://medium.com/@uzzaman.ahmed/pyspark-date-time-functions-a-comprehensive-guide-b250e92df264\n",
        "https://www.chaosgenius.io/blog/databricks-data-types/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-join-multiple-columns/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/  Partitioning Data\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "l1BLdgRW65ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyU4Id4h65be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yf72Moipu1LC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
